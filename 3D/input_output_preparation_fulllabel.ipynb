{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from labeling import *\n",
    "from CO2_identify import resize3d, mute_top\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../define_path.txt'\n",
    "with open(fn) as f:\n",
    "    lines = f.readlines()\n",
    "for idx, line in enumerate(lines):\n",
    "    if idx == 1:\n",
    "        dir_co2 = line.split('=')[1][:-1]\n",
    "    if idx == 3:\n",
    "        dir_grid = line.split('=')[1][:-1]\n",
    "    if idx == 5:\n",
    "        outpath = line.split('=')[1][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basic pathes for baseline data and time-lapse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file pathes\n",
    "blfn = f'{dir_co2}/94p10/2010 processing/data/94p10nea.sgy' # baseline data processed in 2010\n",
    "blfn2 = f'{dir_co2}/94p01/2001 processing/data/94p01nea.sgy' # baseline data processed in 2001\n",
    "tlfn = f'{dir_co2}/10p10/2010 processing/data/10p10nea.sgy' # timelapse (2010) data processed in 2010\n",
    "tlfn2 = f'{dir_co2}/10p11/2011 processing/data/10p11nea' # timelapse (2011) data processed in 2011\n",
    "mkfn = f'../resources/label/masks.dat' # CO2 masks interpreted from 1994 and 2010 data processed in 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data head\n",
    "D0 = dataload(fn=blfn)\n",
    "D0n = dataload(fn=blfn2)\n",
    "Dt = dataload(fn=tlfn)\n",
    "Dtn = dataload(fn=tlfn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask dimension\n",
    "DD = (Dt.nx,Dt.ny,Dt.nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the entire 3D data from the three data volume\n",
    "d0,xd0,yd0,td0 = D0.getdata()\n",
    "d0n,xdn,ydn,_ = D0n.getdata()\n",
    "dt,xd,yd,td = Dt.getdata()\n",
    "dtn,xdt,ydt,tdt = Dtn.getdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess d0n and dtn to make them in the same dimension as d0 and dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the water bottom reflection\n",
    "r'''\n",
    "t1,t2 = 10,20\n",
    "d0 = mute_top(d0,t1,t2)\n",
    "d0n = mute_top(d0n,t1,t2)\n",
    "dt = mute_top(dt,t1,t2)\n",
    "dtn = mute_top(dtn,t1,t2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the relative trace location between the baseline or time-lapse data and the reference data position\n",
    "r'''\n",
    "xm = np.amin(xd)\n",
    "ym = np.amin(yd)\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,10))\n",
    "ax.scatter(xd-xm,yd-ym,c='r')\n",
    "ax.scatter(xd0-xm,yd0-ym,c='b')\n",
    "ax.axis('equal')\n",
    "ax.set_ylim(2000,2200)\n",
    "ax.set_xlim(1200,1400)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate d0n to the same dimension as dt(d0)\n",
    "X = np.stack((xd.flatten(),yd.flatten()),axis=1)\n",
    "Xn = np.stack((xdn.flatten(),ydn.flatten()),axis=1)\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(Xn)\n",
    "distances, ind = nbrs.kneighbors(X)\n",
    "sub = ind2sub(xdn.shape,ind)\n",
    "d0i = d0n[sub[0],sub[1],:]\n",
    "d0i = np.reshape(d0i,d0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate dtn the same dimension as dt(d0)\n",
    "# downsample along xline direction\n",
    "dtn1 = np.pad(dtn[:,1:-3:2,:],((0,0),(2,0),(0,0)),'edge')\n",
    "# calculate the coefficient for linear interpolation\n",
    "xdh,xdth = xd[:,0],xdt[:,0]\n",
    "X = np.stack((xdh,np.zeros(Dt.nx)),axis=1)\n",
    "Xt = np.stack((xdth,np.zeros(Dtn.nx)),axis=1)\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(Xt)\n",
    "distances, ind = nbrs.kneighbors(X)\n",
    "coe = np.flip(distances,axis=1)/np.expand_dims(np.sum(distances,axis=1),1)\n",
    "# linear interpolate along inline direction\n",
    "dti = np.zeros(DD,dtype=np.float32)\n",
    "for i in range(Dt.nx):\n",
    "    dti[i,:,:] = dtn1[ind[i,0],:,:]*coe[i,0]+dtn1[ind[i,1],:,:]*coe[i,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "# normalize d0,dt,d0i,dti\n",
    "d0 = (d0-np.mean(d0))/(np.std(d0)+eps)\n",
    "d0i = (d0i-np.mean(d0i))/(np.std(d0i)+eps)\n",
    "dt = (dt-np.mean(dt))/(np.std(dt)+eps)\n",
    "dti = (dti-np.mean(dti))/(np.std(dti)+eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the comparison between different baseline and time-lapse dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude spectra comparison between all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the four data amplitude spectra\n",
    "A0 = np.mean(np.abs(np.fft.fft(d0[:,:,:])),axis=(0,1))\n",
    "A0i = np.mean(np.abs(np.fft.fft(d0i[:,:,:])),axis=(0,1))\n",
    "At = np.mean(np.abs(np.fft.fft(dt[:,:,:])),axis=(0,1))\n",
    "Ati = np.mean(np.abs(np.fft.fft(dti[:,:,:])),axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amplitude comparison for the entire Inline section (Figure 4)\n",
    "f = np.fft.fftfreq(len(td),td[1]-td[0])\n",
    "nfp = len(td)//2\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.plot(f[:nfp],A0i[:nfp],label=f'94p01')\n",
    "ax.plot(f[:nfp],A0[:nfp],label=f'94p10')\n",
    "ax.plot(f[:nfp],At[:nfp],label=f'10p10')\n",
    "ax.plot(f[:nfp],Ati[:nfp],label=f'10p11')\n",
    "ax.set_xlabel('Frequency (Hz)',fontsize=15)\n",
    "ax.set_ylabel('Amplitude',fontsize=15)\n",
    "ax.set_xlim(0,150)\n",
    "ax.set_ylim(0,100)\n",
    "ax.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveform and amplitude spectra comparison between baseline data processed in different years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inline section comparison\n",
    "InN = 130\n",
    "tl = ['2010','2001']\n",
    "fig,ax = plt.subplots(1,3,figsize=(20,5))\n",
    "ax[0].imshow(d0[InN,:,:].T,aspect=0.5,cmap='gray')\n",
    "ax[1].imshow(d0i[InN,:,:].T,aspect=0.5,cmap='gray')\n",
    "ax[2].imshow(dt[InN,:,:].T,aspect=0.5,cmap='gray')\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel('Xline No.')\n",
    "    ax[i].set_ylabel('Time (2 ms)')\n",
    "    if i<2:\n",
    "        ax[i].set_title(f'1994 baseline, processed in {tl[i]}')\n",
    "    else:\n",
    "        ax[i].set_title(f'2010 time-lapse, processed in {tl[0]}')\n",
    "# amplitude comparison for the entire Inline section\n",
    "f = np.fft.fftfreq(len(td),td[1]-td[0])\n",
    "A0 = np.mean(np.abs(np.fft.fft(d0[InN,:,:])),axis=0)\n",
    "A0i = np.mean(np.abs(np.fft.fft(d0i[InN,:,:])),axis=0)\n",
    "At = np.mean(np.abs(np.fft.fft(dt[InN,:,:])),axis=0)\n",
    "nfp = len(td)//2\n",
    "fig,ax = plt.subplots(1,1,figsize=(15,5))\n",
    "ax.plot(f[:nfp],A0[:nfp],label=f'baseline processed in {tl[0]}')\n",
    "ax.plot(f[:nfp],A0i[:nfp],label=f'baseline processed in {tl[1]}')\n",
    "ax.plot(f[:nfp],At[:nfp],label=f'time-lapse processed in {tl[0]}')\n",
    "ax.set_xlabel('Frequency (Hz)')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace comparison\n",
    "XnN = 200\n",
    "t1 = 0\n",
    "t2 = 1000\n",
    "fig,ax = plt.subplots(1,1,figsize=(15,5))\n",
    "ax.plot(d0[InN,XnN,:],label=f'baseline processed in {tl[0]}')\n",
    "ax.plot(d0i[InN,XnN,:],label=f'baseline processed in {tl[1]}')\n",
    "ax.plot(dt[InN,XnN,:],label=f'time-lapse processed in {tl[0]}')\n",
    "ax.set_xlim(t1,t2)\n",
    "ax.set_xlabel('Time (2 ms)')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveform and amplitude spectra comparison between time-lapse data processed in different years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inline section comparison\n",
    "InN = 130\n",
    "tl = ['2010','2011']\n",
    "fig,ax = plt.subplots(1,3,figsize=(20,5))\n",
    "ax[0].imshow(dt[InN,:,:].T,aspect=0.5,cmap='gray')\n",
    "ax[1].imshow(dti[InN,:,:].T,aspect=0.5,cmap='gray')\n",
    "ax[2].imshow(d0[InN,:,:].T,aspect=0.5,cmap='gray')\n",
    "for i in range(3):\n",
    "    ax[i].set_xlabel('Xline No.')\n",
    "    ax[i].set_ylabel('Time (2 ms)')\n",
    "    if i<2:\n",
    "        ax[i].set_title(f'2010 time-lapse, processed in {tl[i]}')\n",
    "    else:\n",
    "        ax[i].set_title(f'1994 baseline, processed in {tl[0]}')\n",
    "# amplitude comparison for the entire Inline section\n",
    "f = np.fft.fftfreq(len(td),td[1]-td[0])\n",
    "At = np.mean(np.abs(np.fft.fft(dt[InN,:,:])),axis=0)\n",
    "Ati = np.mean(np.abs(np.fft.fft(dti[InN,:,:])),axis=0)\n",
    "A0 = np.mean(np.abs(np.fft.fft(d0[InN,:,:])),axis=0)\n",
    "nfp = len(td)//2\n",
    "fig,ax = plt.subplots(1,1,figsize=(15,5))\n",
    "ax.plot(f[:nfp],At[:nfp],label=f'time-lapse processed in {tl[0]}')\n",
    "ax.plot(f[:nfp],Ati[:nfp],label=f'time-lapse processed in {tl[1]}')\n",
    "ax.plot(f[:nfp],A0[:nfp],label=f'baseline processed in {tl[0]}')\n",
    "ax.set_xlabel('Frequency (Hz)')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace comparison\n",
    "XnN = 200\n",
    "t1 = 0\n",
    "t2 = 1000\n",
    "fig,ax = plt.subplots(1,1,figsize=(15,5))\n",
    "ax.plot(dt[InN,XnN,:],label=f'time-lapse processed in {tl[0]}')\n",
    "ax.plot(dti[InN,XnN,:],label=f'time-lapse processed in {tl[1]}')\n",
    "ax.plot(d0[InN,XnN,:],label=f'baseline processed in {tl[0]}')\n",
    "ax.set_xlim(t1,t2)\n",
    "ax.set_xlabel('Time (2 ms)')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate 3D patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the reference CO2 mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readin CO2 mask\n",
    "masks = np.fromfile(f'{mkfn}',dtype=np.float32)\n",
    "masks = np.reshape(masks,DD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define basic cube dimension and sampling parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the patch size\n",
    "Ni,Nx,Nt = 128,128,256\n",
    "# define the sampled number along x, y and t\n",
    "Nsi,Nsx,Nst = 8,9,10\n",
    "# define resized dimension of the 3D patch\n",
    "rs = (64,64,64)\n",
    "# sample strategy indicator\n",
    "stg = 0 # 0-random; 1-regular\n",
    "# normalize patch\n",
    "eps = 1e-5 # dividing std stablizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to use different baseline data\n",
    "diffbl = True\n",
    "# whether to use different time-lapse data\n",
    "difftl = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the patch No.\n",
    "if diffbl:\n",
    "    NRb = 2\n",
    "else:\n",
    "    NRb = 1\n",
    "if difftl:\n",
    "    NRa = 2\n",
    "else:\n",
    "    NRa = 1\n",
    "print(f'Theoritical maximum total patch number: {NRb*NRa*Nsi*Nsx*Nst}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wether to use slice interpretation\n",
    "bool_sliceitp = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of patches with no co2 marks\n",
    "max_0co2 = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate parameter csv\n",
    "pm_info = OrderedDict()\n",
    "pm_info['data_dim'] = [D0.nx,D0.ny,D0.nt] # [ensemble number,trace number per ensemble,sample number per trace]\n",
    "if diffbl:\n",
    "    pm_info['baseline_datapath'] = [blfn,blfn2]\n",
    "else:\n",
    "    pm_info['baseline_datapath'] = [blfn]\n",
    "if difftl:\n",
    "    pm_info['timelapse_datapath'] = [tlfn,tlfn2]\n",
    "else:\n",
    "    pm_info['timelapse_datapath'] = [tlfn]\n",
    "pm_info['patch_osize'] = [Ni,Nx,Nt]\n",
    "pm_info['patch_nsize'] = rs\n",
    "pm_info['patch_number'] = [Nsi,Nsx,Nst]\n",
    "pm_info['sample_strategy'] = stg\n",
    "pm_info['max_0co2'] = max_0co2 # if none, means no limit for zero co2 masks\n",
    "with open(f'{outpath}/pm_info.json','w') as f:\n",
    "    f.write(json.dumps(pm_info))\n",
    "with open(f'{outpath}/pm_info.json','r') as read_file:\n",
    "    loaded_pm = json.loads(read_file.read())\n",
    "    print(loaded_pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample cube centers in the 3D dataset volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling center range along x, y, and t directions\n",
    "hNi = Ni//2\n",
    "hNx = Nx//2\n",
    "hNt = Nt//2\n",
    "Ir = [hNi,D0.nx-hNi-1]\n",
    "Xr = [hNx,D0.ny-hNx-1]\n",
    "Tr = [hNt,D0.nt-hNt]\n",
    "# generate random/or regular samples along t and r\n",
    "if stg == 1:\n",
    "    Is = np.array(np.linspace(Ir[0],Ir[1],Nsi),dtype=np.int16)\n",
    "    Xs = np.array(np.linspace(Xr[0],Xr[1],Nsx),dtype=np.int16)\n",
    "    Ts = np.array(np.linspace(Tr[0],Tr[1],Nst),dtype=np.int16)\n",
    "    Isample,Xsample,Tsample = np.meshgrid(Is,Xs,Ts,indexing='ij')\n",
    "else:\n",
    "    Isample = np.random.randint(Ir[0],Ir[1],size=(Nsi,Nsx,Nst))\n",
    "    Xsample = np.random.randint(Xr[0],Xr[1],size=(Nsi,Nsx,Nst))\n",
    "    Tsample = np.random.randint(Tr[0],Tr[1],size=(Nsi,Nsx,Nst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise some Tsample to make them sample the bottom image part\n",
    "Rbm = 20/(Nsi*Nsx*Nst) # ratio of samples being revised to Tr[-1]\n",
    "Tpf = Tsample.flatten()\n",
    "Tpf[:int(Nsi*Nsx*Nst*Rbm)] = Tr[-1]\n",
    "Tsample = np.reshape(Tpf,(Nsi,Nsx,Nst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise some Isample to make them sample the top image part\n",
    "Rtp = 20/(Nsi*Nsx*Nst) # ratio of samples being revised to Tr[0]\n",
    "Tpf = Tsample.flatten()\n",
    "if Rtp != 0:\n",
    "    Tpf[-int(Nsi*Nsx*Nst*Rtp):] = Tr[0]\n",
    "Tsample = np.reshape(Tpf,(Nsi,Nsx,Nst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the Isample and Xsample even\n",
    "Isample = Isample//2*2\n",
    "Xsample = Xsample//2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "# show the sample positions in 3D view\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(Isample, Xsample, Tsample, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d resize class\n",
    "RS = resize3d(1,D=rs[0],H=rs[0],W=rs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the patches iteratively\n",
    "hNi1 = Ni-hNi\n",
    "hNx1 = Nx-hNx\n",
    "hNt1 = Nt-hNt\n",
    "patch_info = []\n",
    "c = 0\n",
    "cm = 0\n",
    "c0 = 0\n",
    "for i,j,k in product(range(Nst),range(Nsx),range(Nsi)):\n",
    "    Is = Isample[k,j,i]\n",
    "    Xs = Xsample[k,j,i]\n",
    "    Ts = Tsample[k,j,i]\n",
    "    # slice the data\n",
    "    R0 = d0[Is-hNi:Is+hNi1,Xs-hNx:Xs+hNx1,Ts-hNt:Ts+hNt1]\n",
    "    R0i = d0i[Is-hNi:Is+hNi1,Xs-hNx:Xs+hNx1,Ts-hNt:Ts+hNt1]\n",
    "    Rt = dt[Is-hNi:Is+hNi1,Xs-hNx:Xs+hNx1,Ts-hNt:Ts+hNt1]\n",
    "    Rti = dti[Is-hNi:Is+hNi1,Xs-hNx:Xs+hNx1,Ts-hNt:Ts+hNt1]\n",
    "    # slice the mask\n",
    "    if bool_sliceitp:\n",
    "        M = np.array(mask_itp[Is-hNi:Is+hNi1,Xs-hNx:Xs+hNx1,Ts-hNt:Ts+hNt1],dtype=np.float32)\n",
    "        M = torch.tensor(M[::2,::2,::4]).unsqueeze(0).unsqueeze(0).numpy()\n",
    "        #M = RS.resize(torch.tensor(M).unsqueeze(0).unsqueeze(0),NP=True) # 5D: (N=1,C=1,D=64,H=64,W=64)\n",
    "        W = np.array(weight[Is-hNi:Is+hNi1,Xs-hNx:Xs+hNx1,Ts-hNt:Ts+hNt1],dtype=np.float32)\n",
    "        W = torch.tensor(W[::2,::2,::4]).unsqueeze(0).unsqueeze(0).numpy()\n",
    "        #W = RS.resize(torch.tensor(W).unsqueeze(0).unsqueeze(0),NP=True) # 5D: (N=1,C=1,D=64,H=64,W=64)\n",
    "    else:\n",
    "        M = np.array(masks[Is-hNi:Is+hNi1,Xs-hNx:Xs+hNx1,Ts-hNt:Ts+hNt1],dtype=np.float32)\n",
    "        M = RS.resize(torch.tensor(M).unsqueeze(0).unsqueeze(0),NP=True) # 5D: (N=1,C=1,D=64,H=64,W=64)        \n",
    "    if max_0co2 is not None:\n",
    "        if np.sum(M) == 0:\n",
    "            if Is!=Ir[0]:\n",
    "                if c0<max_0co2:\n",
    "                    c0 += 1\n",
    "                    if diffbl:\n",
    "                        c0 += 1\n",
    "                else:\n",
    "                    continue\n",
    "    Rb = [R0,R0i]\n",
    "    Ra = [Rt,Rti]\n",
    "    for tyb,tya in product(range(NRb),range(NRa)):\n",
    "        Rb0 = Rb[tyb]\n",
    "        Ra0 = Ra[tya]\n",
    "        # normalize R0 and Rt respectively\n",
    "        Rm0,Rs0 = np.mean(Rb0),np.std(Rb0)\n",
    "        Rmt,Rst = np.mean(Ra0),np.std(Ra0)\n",
    "        Rb0 = (Rb0-Rm0)/(Rs0+eps)\n",
    "        Ra0 = (Ra0-Rmt)/(Rst+eps)\n",
    "        # stack R0 and Rt\n",
    "        R0t = torch.tensor(np.stack((Rb0,Ra0))).unsqueeze(0)\n",
    "        R0t = RS.resize(R0t,NP=True) # 5D: (N=1,C=2,D=64,H=64,W=64)\n",
    "        # save the patches and corresponding masks to outpath\n",
    "        R0t.tofile(f'{outpath}/R0t_{c}.dat')\n",
    "        # record the patch information in patch_info dict\n",
    "        pf = OrderedDict()\n",
    "        pf['Ptch_id'] = c\n",
    "        pf['Mask_id'] = cm\n",
    "        pf['ct'] = [Is,Xs,Ts]\n",
    "        pf['mean'] = [Rm0,Rmt]\n",
    "        pf['std'] = [Rs0,Rst]\n",
    "        patch_info.append(pf)\n",
    "        c += 1\n",
    "    M.tofile(f'{outpath}/Mask_{cm}.dat')\n",
    "    if bool_sliceitp:\n",
    "        W.tofile(f'{outpath}/Weight_{cm}.dat')\n",
    "    cm += 1\n",
    "# save the patch info\n",
    "pd.DataFrame.from_dict(patch_info, orient='columns').to_csv(f'{outpath}/patch_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total patch number: {len(patch_info)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate validating dataset by randomly selecting a small portion of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntr = 1500\n",
    "N = len(patch_info)\n",
    "vmax = N-Ntr\n",
    "pv = 1.1*vmax/len(patch_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patch_info_train = []\n",
    "patch_info_valid = []\n",
    "x = np.random.random(N)\n",
    "c = 0\n",
    "for i in range(N):\n",
    "    if (x[i]<=pv) and (c<vmax):\n",
    "        patch_info_valid.append(patch_info[i])\n",
    "        c += 1\n",
    "    else:\n",
    "        patch_info_train.append(patch_info[i])     \n",
    "# save the patch info\n",
    "pd.DataFrame.from_dict(patch_info_train, orient='columns').to_csv(f'{outpath}/patch_info_train.csv')\n",
    "pd.DataFrame.from_dict(patch_info_valid, orient='columns').to_csv(f'{outpath}/patch_info_valid.csv')\n",
    "# print the number of training and validating\n",
    "print(f'Final patch number for training: {len(patch_info_train)}')\n",
    "print(f'Final patch number for validating: {len(patch_info_valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../define_path.txt'\n",
    "with open(fn) as f:\n",
    "    lines = f.readlines()\n",
    "for idx, line in enumerate(lines):\n",
    "    if idx == 9:\n",
    "        outpath_test = line.split('=')[1][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basic information for test dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference dataset grid path\n",
    "xydfn = f'{dir_co2}/10p10/2010 processing/data/10p10nea.sgy'\n",
    "# load the reference dataset head\n",
    "Dr = dataload(fn=xydfn)\n",
    "# get the grid axes\n",
    "_,xd,yd,td = Dr.getdata()\n",
    "# grid dimension\n",
    "DD = (xd.shape[0],xd.shape[1],len(td))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline processing year\n",
    "bpy = ['2001','2001','2001','2001','2001','2001','2001','2010','2010']\n",
    "# time-lapse data year\n",
    "ty = ['1999','2001','2004','2006','2008','2010','2010','2010','2010']\n",
    "# time-lapse processing year\n",
    "tpy = ['2001','2001','2007','2007','2008','2010','2011','2010','2011']\n",
    "# number of tests\n",
    "Nte = len(bpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the patch size\n",
    "Ni,Nx,Nt = 128,128,256\n",
    "# define the sampled number along x, y and t\n",
    "Nsi,Nsx,Nst = 3,5,6\n",
    "# define resized dimension of the 3D patch\n",
    "rs = (64,64,64)\n",
    "# sample strategy indicator\n",
    "stg = 1 # 0-random; 1-regular\n",
    "# normalize patch\n",
    "eps = 1e-5 # dividing std stablizer\n",
    "# patch No.\n",
    "print(f'Total patch number: {Nsi*Nsx*Nst}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readin reference CO2 mask for 2010\n",
    "mkfn = f'../resources/label/masks.dat'\n",
    "masks = np.fromfile(f'{mkfn}',dtype=np.float32)\n",
    "masks = np.reshape(masks,DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling center range along x, y, and t directions\n",
    "hNi = Ni//2\n",
    "hNx = Nx//2\n",
    "hNt = Nt//2\n",
    "hNi1 = Ni-hNi\n",
    "hNx1 = Nx-hNx\n",
    "hNt1 = Nt-hNt\n",
    "Ir = [hNi,DD[0]-hNi]\n",
    "Xr = [hNx,DD[1]-hNx]\n",
    "Tr = [hNt,DD[2]-hNt]\n",
    "# generate random/or regular samples along t and r\n",
    "if stg == 1:\n",
    "    Is = np.array(np.linspace(Ir[0],Ir[1],Nsi),dtype=np.int16)\n",
    "    Xs = np.array(np.linspace(Xr[0],Xr[1],Nsx),dtype=np.int16)\n",
    "    Ts = np.array(np.linspace(Tr[0],Tr[1],Nst),dtype=np.int16)\n",
    "    Isample,Xsample,Tsample = np.meshgrid(Is,Xs,Ts,indexing='ij')\n",
    "else:\n",
    "    Isample = np.random.randint(Ir[0],Ir[1],size=(Nsi,Nsx,Nst))\n",
    "    Xsample = np.random.randint(Xr[0],Xr[1],size=(Nsi,Nsx,Nst))\n",
    "    Tsample = np.random.randint(Tr[0],Tr[1],size=(Nsi,Nsx,Nst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d resize class\n",
    "RS = resize3d(1,D=rs[0],H=rs[0],W=rs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test datasets for all years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for I in range(Nte):\n",
    "    # file pathes\n",
    "    blfn = f'{dir_co2}/94p{bpy[I][-2:]}/{bpy[I]} processing/data/94p{bpy[I][-2:]}nea.sgy'\n",
    "    tlfn = f'{dir_co2}/{ty[I][-2:]}p{tpy[I][-2:]}/{tpy[I]} processing/data/{ty[I][-2:]}p{tpy[I][-2:]}nea.sgy'\n",
    "    year = f'{ty[I]}_b{bpy[I][-2:]}_t{tpy[I][-2:]}'\n",
    "    # output dataset path\n",
    "    outpath = f'{outpath_test}/{year}/test'\n",
    "    if tpy[I] == '2011':\n",
    "        tlfn = tlfn[:-4]\n",
    "    # load data\n",
    "    D0 = dataload(fn=blfn)\n",
    "    Dt = dataload(fn=tlfn)\n",
    "    d0,xd0,yd0,_ = D0.getdata()\n",
    "    dt,xdt,ydt,_ = Dt.getdata()\n",
    "    \n",
    "    ## preprocess the datasets\n",
    "    # interpolate d0\n",
    "    X = np.stack((xd.flatten(),yd.flatten()),axis=1)\n",
    "    X0 = np.stack((xd0.flatten(),yd0.flatten()),axis=1)\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(X0)\n",
    "    distances, ind = nbrs.kneighbors(X)\n",
    "    sub = ind2sub(xd0.shape,ind)\n",
    "    d0i = d0[sub[0],sub[1],:]\n",
    "    d0i = np.reshape(d0i,DD)\n",
    "    d0 = d0i\n",
    "    # interpolate dt\n",
    "    if tpy[I] == '2011':\n",
    "        # interpolate for dt to conform to Dr\n",
    "        # downsample along xline direction\n",
    "        dt1 = np.pad(dt[:,1:-3:2,:],((0,0),(2,0),(0,0)),'edge')\n",
    "        # calculate the coefficient for linear interpolation\n",
    "        xdh,xdth = xd[:,0],xdt[:,0]\n",
    "        X = np.stack((xdh,np.zeros(Dr.nx)),axis=1)\n",
    "        Xt = np.stack((xdth,np.zeros(Dt.nx)),axis=1)\n",
    "        nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(Xt)\n",
    "        distances, ind = nbrs.kneighbors(X)\n",
    "        coe = np.flip(distances,axis=1)/np.expand_dims(np.sum(distances,axis=1),1)\n",
    "        # linear interpolate along inline direction\n",
    "        dti = np.zeros(DD,dtype=np.float32)\n",
    "        for i in range(Dr.nx):\n",
    "            dti[i,:,:] = dt1[ind[i,0],:,:]*coe[i,0]+dt1[ind[i,1],:,:]*coe[i,1]\n",
    "        dt = dti\n",
    "    else:\n",
    "        # find nearest trace of d0 from (xd,yd)\n",
    "        X = np.stack((xd.flatten(),yd.flatten()),axis=1)\n",
    "        Xt = np.stack((xdt.flatten(),ydt.flatten()),axis=1)\n",
    "        nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(Xt)\n",
    "        distances, ind = nbrs.kneighbors(X)\n",
    "        sub = ind2sub(xdt.shape,ind)\n",
    "        dti = dt[sub[0],sub[1],:]\n",
    "        dti = np.reshape(dti,DD)\n",
    "        dt = dti\n",
    "    # whether there is reference mask\n",
    "    if ty[I] == '2010':\n",
    "        mask_ref = True\n",
    "    else:\n",
    "        mask_ref = False\n",
    "    \n",
    "    # generate parameter csv\n",
    "    pm_info = OrderedDict()\n",
    "    pm_info['data_dim'] = DD\n",
    "    pm_info['data_path'] = [blfn,tlfn]\n",
    "    pm_info['patch_osize'] = [Ni,Nx,Nt]\n",
    "    pm_info['patch_nsize'] = rs\n",
    "    pm_info['patch_number'] = [Nsi,Nsx,Nst]\n",
    "    pm_info['sample_strategy'] = stg\n",
    "    pm_info['mask_ref'] = mask_ref\n",
    "    with open(f'{outpath}/pm_info.json','w') as f:\n",
    "        f.write(json.dumps(pm_info))\n",
    "    with open(f'{outpath}/pm_info.json','r') as read_file:\n",
    "        loaded_pm = json.loads(read_file.read())\n",
    "        print(loaded_pm)\n",
    "    # normalize d0 and dt\n",
    "    d0N = (d0-np.mean(d0))/(np.std(d0)+eps)\n",
    "    dtN = (dt-np.mean(dt))/(np.std(dt)+eps)\n",
    "    # save d0 and dt to outpath\n",
    "    d0N.tofile(f'{outpath}/d0.dat')\n",
    "    dtN.tofile(f'{outpath}/dt.dat')\n",
    "    # generate patches\n",
    "    patch_info = []\n",
    "    c = 0\n",
    "    for i,j,k in product(range(Nsi),range(Nsx),range(Nst)):\n",
    "        Is = Isample[i,j,k]\n",
    "        Xs = Xsample[i,j,k]\n",
    "        Ts = Tsample[i,j,k]\n",
    "        # slice the data\n",
    "        R0 = d0N[Is-hNi:Is+hNi1,Xs-hNx:Xs+hNx1,Ts-hNt:Ts+hNt1]\n",
    "        Rt = dtN[Is-hNi:Is+hNi1,Xs-hNx:Xs+hNx1,Ts-hNt:Ts+hNt1]\n",
    "        if mask_ref:\n",
    "            # slice the mask\n",
    "            M = np.array(masks[Is-hNi:Is+hNi1,Xs-hNx:Xs+hNx1,Ts-hNt:Ts+hNt1],dtype=np.float32)\n",
    "            M = RS.resize(torch.tensor(M).unsqueeze(0).unsqueeze(0),NP=True) # 5D: (N=1,C=1,D=128,H=128,W=128)\n",
    "        Rb0 = R0\n",
    "        # normalize R0 and Rt respectively\n",
    "        Rm0,Rs0 = np.mean(Rb0),np.std(Rb0)\n",
    "        Rmt,Rst = np.mean(Rt),np.std(Rt)\n",
    "        Rb0 = (Rb0-Rm0)/(Rs0+eps)\n",
    "        Rt = (Rt-Rmt)/(Rst+eps)\n",
    "        # stack R0 and Rt\n",
    "        R0t = torch.tensor(np.stack((Rb0,Rt))).unsqueeze(0)\n",
    "        R0t = RS.resize(R0t,NP=True) # 5D: (N=1,C=2,D=64,H=64,W=64)\n",
    "        # save the patches and corresponding masks to outpath\n",
    "        R0t.tofile(f'{outpath}/R0t_{c}.dat')\n",
    "        # record the patch information in patch_info dict\n",
    "        pf = OrderedDict()\n",
    "        pf['Ptch_id'] = c\n",
    "        pf['Mask_id'] = c\n",
    "        pf['ct'] = [Is,Xs,Ts]\n",
    "        pf['mean'] = [Rm0,Rmt]\n",
    "        pf['std'] = [Rs0,Rst]\n",
    "        patch_info.append(pf)\n",
    "        if mask_ref:\n",
    "            M.tofile(f'{outpath}/Mask_{c}.dat')\n",
    "        c += 1\n",
    "        \n",
    "    # save the patch info\n",
    "    pd.DataFrame.from_dict(patch_info, orient='columns').to_csv(f'{outpath}/patch_info.csv')\n",
    "    print(f'///////////////////////////////////////////////////')\n",
    "    print(f'Final patch number for {year}: {len(patch_info)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265.883px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
