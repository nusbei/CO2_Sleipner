{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from labeling import dataload, show3D\n",
    "from CO2_identify import *\n",
    "from mynetwork import CO2mask\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from torchstat import stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../define_path.txt'\n",
    "with open(fn) as f:\n",
    "    lines = f.readlines()\n",
    "for idx, line in enumerate(lines):\n",
    "    if idx == 1:\n",
    "        dir_co2 = line.split('=')[1][:-1]\n",
    "    if idx == 3:\n",
    "        dir_grid = line.split('=')[1][:-1]\n",
    "    if idx == 13:\n",
    "        root = line.split('=')[1][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN for CO2 mask identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset information file names\n",
    "pmf = 'pm_info.json'\n",
    "pdf = 'patch_info'\n",
    "pdfap = '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "train = dataset_patch(root,pmf,f'{pdf}_train{pdfap}')\n",
    "Ntrain = len(train)\n",
    "print(f'Training dataset size: {Ntrain}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validating dataset\n",
    "valid = dataset_patch(root,pmf,f'{pdf}_valid{pdfap}')\n",
    "Nvalid = len(valid)\n",
    "print(f'Validating dataset size: {Nvalid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network path\n",
    "path_net = f'../resources/NNpred2D/co2_identify.pt'\n",
    "path_bestnet = f'../resources/NNpred2D/co2_identify_best.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display sampled patches in training and validating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sampled patches in training dataset for display\n",
    "ndis_tr = 5\n",
    "train_id_list = np.random.choice(len(train),size=ndis_tr,replace=False)\n",
    "#train_id_list = np.linspace(0,Ntrain,ndis_tr+2,dtype=np.int16)[1:-1]\n",
    "pst = patch_show(train,train_id_list)\n",
    "print(f'train_id_list: {train_id_list}')\n",
    "# define the sampled patches in valid dataset for display\n",
    "ndis_va = 3\n",
    "valid_id_list = np.random.choice(len(valid),size=ndis_va,replace=False)\n",
    "#valid_id_list = np.linspace(0,Nvalid,ndis_va+2,dtype=np.int16)[1:-1]\n",
    "psv = patch_show(valid,valid_id_list)\n",
    "print(f'valid_id_list: {valid_id_list}')\n",
    "# data patch resize shape\n",
    "rs = valid.nsz\n",
    "# sampling rate of epoch for display\n",
    "epoch_itv = 20\n",
    "# sampling rate of batch number for display\n",
    "batch_itv = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pst.view2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "psv.view2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = OrderedDict(\n",
    "    lr = [.0002]\n",
    "    ,batch_size = [30]\n",
    "    ,shuffle = [True]\n",
    "    ,epoch_num = [200]\n",
    "    ,adadelta_num = [0]\n",
    ")\n",
    "cuda_gpu = True\n",
    "gpus = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize run\n",
    "M = RunManager(cuda_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sampler for loading valid_set\n",
    "#valid_sampler = SubsetSampler(valid_id_list)\n",
    "cpu_device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in RunBuilder.get_runs(params):\n",
    "    # initialize network\n",
    "    nw = 2\n",
    "    network = CO2mask()\n",
    "    \n",
    "    # inherit from previous train network\n",
    "    #network.load_state_dict(torch.load(path_bestnet,map_location=cpu_device))\n",
    "    #network = network.train()\n",
    "\n",
    "    if cuda_gpu:\n",
    "        network = torch.nn.DataParallel(network, device_ids=gpus).cuda()\n",
    "        nw = 0\n",
    "    # train_set loader\n",
    "    loader_train = DataLoader(\n",
    "        train\n",
    "        ,batch_size = run.batch_size\n",
    "        ,shuffle = run.shuffle\n",
    "        ,num_workers = nw\n",
    "        ,drop_last = False)\n",
    "    # valid_set loader (load the entire dataset as a single batch)\n",
    "    loader_valid = DataLoader(\n",
    "         valid\n",
    "        ,batch_size = Nvalid\n",
    "        ,shuffle = False\n",
    "        ,num_workers = nw\n",
    "        ,drop_last = False)\n",
    "    \n",
    "    # define the initial optimizer as adadelta\n",
    "    optimizer = optim.Adadelta(network.parameters())\n",
    "\n",
    "    # initialize training and validation loss lists\n",
    "    TrLoss_list = []\n",
    "    VaLoss_list = []\n",
    "    Bloss = float('inf') # initial smallest loss\n",
    "    Bloss_epNo = 0 # intial smallest loss corresponding epoch No.\n",
    "    \n",
    "    # begin this training run\n",
    "    M.begin_run(run, network, loader_train)\n",
    "    for epoch in range(run.epoch_num):\n",
    "        print(f'Epoch No.: {epoch}')\n",
    "        # initialize runner\n",
    "        M.begin_epoch()\n",
    "        batch_id=0\n",
    "        Loss = 0\n",
    "        \n",
    "        # adjust learning rate\n",
    "        #adjust_learning_rate(optimizer, epoch, run.lr)\n",
    "        \n",
    "        # initialize sampled prediction arrays\n",
    "        Trpred = np.zeros((ndis_tr,rs[0],rs[1]))\n",
    "        Vapred = np.zeros((ndis_va,rs[0],rs[1]))\n",
    "        \n",
    "        if epoch == run.adadelta_num:\n",
    "            # redefine the optimizer as adam \n",
    "            optimizer = optim.Adam(network.parameters(),run.lr)\n",
    "\n",
    "        # loop through different batches in training dataset\n",
    "        Loss = 0 # initial loss\n",
    "        Np = 0 # number of patches\n",
    "        C = 0 # batch No.\n",
    "        for batch in loader_train:\n",
    "            if C%batch_itv == 0:\n",
    "                print(f'Batch No. {C}--------------------')\n",
    "            C += 1\n",
    "            R0t, Mask, idx = batch\n",
    "            # find the indices of sampled training patches in current batch for later display\n",
    "            bs = len(idx)\n",
    "            Np += bs\n",
    "            Idx = idx.tolist()\n",
    "            cp = findtrace(train_id_list,Idx)\n",
    "            # copy cpu data on GPU\n",
    "            if cuda_gpu:\n",
    "                R0t = R0t.cuda()\n",
    "                Mask = Mask.cuda()\n",
    "            # forward modeling\n",
    "            pMask = network(R0t)\n",
    "            \n",
    "            # record the sampled training patches for later display\n",
    "            for c,p in cp:\n",
    "                Trpred[c] = pMask[p][0].cpu().detach().numpy()\n",
    "            # validating loss\n",
    "            loss = F.binary_cross_entropy(pMask, Mask)\n",
    "            #loss = BBCE(pMask, Mask)\n",
    "            # backward for gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update NN\n",
    "            optimizer.step()\n",
    "            # track the loss\n",
    "            Loss = track_loss_out(Loss,loss,bs)\n",
    "\n",
    "        # record the mean loss for the entire training dataset\n",
    "        TrLoss_list.append(Loss/Np)\n",
    "        print(f'Mean training loss for epoch No. {epoch}: {Loss/Np}')\n",
    "        # display the sampled patch fitting in training dataset\n",
    "        if (epoch%epoch_itv == 0) or (epoch == run.epoch_num-1):\n",
    "            print(f'Training patch samples display at epoch No. {epoch}')\n",
    "            pst.view2d(Trpred)\n",
    "        \n",
    "        # save the current-epoch training network\n",
    "        torch.save(network.module.state_dict(),path_net)\n",
    "        \n",
    "        r'''\n",
    "        (for cpu validating)\n",
    "        # Load saved validating network on cpu\n",
    "        networkvalid = CO2mask()\n",
    "        networkvalid.load_state_dict(torch.load(path_net,map_location=cpu_device))\n",
    "        '''\n",
    "        \n",
    "        networkvalid = network.eval()\n",
    "        print(f'Start validating for epoch No. {epoch}')\n",
    "        # loop through different batches in valid dataset\n",
    "        Loss = 0 # initial loss\n",
    "        Np = 0 # number of patches\n",
    "        for batch in loader_valid:\n",
    "            R0t, Mask, idx = batch\n",
    "            # find the indices of sampled validating patches in current batch for later display\n",
    "            bs = len(idx)\n",
    "            Np += bs\n",
    "            Idx = idx.tolist()\n",
    "            cp = findtrace(valid_id_list,Idx)\n",
    "            # copy cpu data on GPU\n",
    "            if cuda_gpu:\n",
    "                R0t = R0t.cuda()\n",
    "                Mask = Mask.cuda()\n",
    "            # forward modeling\n",
    "            with torch.no_grad():\n",
    "                pMask = networkvalid(R0t)\n",
    "            # record the sampled validating patches for later display\n",
    "            for c,p in cp:\n",
    "                Vapred[c] = pMask[p][0].detach().cpu().numpy()   \n",
    "            # valid loss\n",
    "            loss = F.binary_cross_entropy(pMask, Mask)\n",
    "            #loss = BBCE(pMask, Mask)\n",
    "            # track the loss\n",
    "            Loss = track_loss_out(Loss,loss,bs)\n",
    "\n",
    "        # record the mean loss for the entire validating dataset\n",
    "        VaLoss_list.append(Loss/Np)\n",
    "        print(f'Mean validating loss for epoch No. {epoch}: {Loss/Np}')\n",
    "        if epoch>0:\n",
    "            if VaLoss_list[-1] < Bloss:\n",
    "                # save the currently best network (providing smallest validating loss)\n",
    "                torch.save(network.module.state_dict(),path_bestnet)\n",
    "                Bloss = VaLoss_list[-1]\n",
    "                Bloss_epNo = epoch\n",
    "                \n",
    "        # display the validating result\n",
    "        if (epoch%epoch_itv == 0) or (epoch == run.epoch_num-1):    \n",
    "            print(f'Validating patch samples display at epoch No. {epoch}')\n",
    "            psv.view2d(Vapred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training time: {te-ts} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validating loss\n",
    "epoch = np.arange(run.epoch_num)\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(epoch,TrLoss_list,label='Training')\n",
    "ax.plot(epoch,VaLoss_list,label='Validating')\n",
    "ax.plot(Bloss_epNo,Bloss,'ro',label='Best Validating result')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_ylim(0,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(TrLoss_list,dtype=np.float32).tofile(f'../resources/NNpred2D/train_loss.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(VaLoss_list,dtype=np.float32).tofile(f'../resources/NNpred2D/valid_loss.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../define_path.txt'\n",
    "with open(fn) as f:\n",
    "    lines = f.readlines()\n",
    "for idx, line in enumerate(lines):\n",
    "    if idx == 1:\n",
    "        dir_co2 = line.split('=')[1][:-1]\n",
    "    if idx == 15:\n",
    "        testpath = line.split('=')[1][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the network and basic information for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing datasets\n",
    "yearlist = ['1999_b01_t01','2001_b01_t01',\n",
    "            '2004_b01_t07','2006_b01_t07',\n",
    "            '2008_b01_t08',\n",
    "            '2010_b01_t10','2010_b01_t11','2010_b10_t10','2010_b10_t11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference dataset grid path\n",
    "xydfn = f'{dir_co2}/10p10/2010 processing/data/10p10nea.sgy'\n",
    "# load the reference dataset head\n",
    "Dr = dataload(fn=xydfn)\n",
    "DD = (Dr.nx,Dr.ny,Dr.nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network path\n",
    "path_bestnet = f'../resources/NNpred2D/co2_identify.pt'\n",
    "gpus = [0]\n",
    "cuda_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network\n",
    "networktest = CO2mask()\n",
    "networktest.load_state_dict(torch.load(path_bestnet,map_location=torch.device('cpu')))\n",
    "networktest = networktest.eval()\n",
    "if cuda_gpu:\n",
    "    networktest = torch.nn.DataParallel(networktest, device_ids=gpus).cuda()\n",
    "    nw = 0\n",
    "networktest = networktest.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed dataset information file names\n",
    "pmf = 'pm_info.json'\n",
    "pdf = 'patch_info.csv'\n",
    "# batch_size of testing dataset\n",
    "bs = 1000\n",
    "# sampling number for patch display\n",
    "ndis_ts = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference mask dataset for training and validating\n",
    "mkfn = f'../resources/label/masks.dat'\n",
    "# readin CO2 mask\n",
    "masks = np.fromfile(f'{mkfn}',dtype=np.float32)\n",
    "masks = np.reshape(masks,DD)\n",
    "# find the slice indices for display\n",
    "MI = np.argmax(np.sum(masks,axis=(1,2)))\n",
    "MX = np.argmax(np.sum(masks,axis=(0,2)))\n",
    "MT = np.argmax(np.sum(masks,axis=(0,1)))\n",
    "MIXT = (MI,MX,MT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start to test dataset for all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in yearlist:\n",
    "    print(f'Testing year: {year}')\n",
    "    # load test dataset\n",
    "    root_test = f'{testpath}/{year}/test'\n",
    "    if year[:4] == '2010':\n",
    "        maskyear = True\n",
    "    else:\n",
    "        maskyear = False\n",
    "    test = dataset_patch(root_test,pmf,pdf,mask=maskyear)\n",
    "    Ntest = len(test)\n",
    "    print(f'Testing dataset size: {Ntest}')\n",
    "    # define the sampled patches in test dataset for display\n",
    "    #test_id_list = np.random.choice(len(test),size=ndis_ts,replace=False)\n",
    "    test_id_list = np.linspace(0,Ntest,ndis_ts+2,dtype=np.int16)[1:-1]\n",
    "    pss = patch_show(test,test_id_list)\n",
    "    print(f'test_id_list for {year}: {test_id_list}')\n",
    "    \n",
    "    # display the test patches\n",
    "    #pss.view2d()\n",
    "    \n",
    "    # patch size\n",
    "    rs = test.nsz\n",
    "    # loop through different batches in testing dataset\n",
    "    loader_test = DataLoader(\n",
    "         test\n",
    "        ,batch_size = bs\n",
    "        ,drop_last = False)\n",
    "    # allocate memory for testing batches\n",
    "    Tepred = np.zeros((ndis_ts,rs[0],rs[1]))\n",
    "    teMasks = torch.zeros((Ntest,1,rs[0],rs[1]),dtype=torch.float32)\n",
    "    Np = 0 # current accumulative number of patches\n",
    "    for batch in loader_test:\n",
    "        if test.mask:\n",
    "            R0t, _, idx = batch\n",
    "        else:\n",
    "            R0t, idx = batch\n",
    "        # copy cpu data on GPU\n",
    "        if cuda_gpu:\n",
    "            R0t = R0t.cuda()\n",
    "        # forward modeling\n",
    "        bs = len(idx)\n",
    "        Np += bs\n",
    "        # find the indices of sampled testing patches in current batch for later display\n",
    "        Idx = idx.tolist()\n",
    "        cp = findtrace(test_id_list,Idx)\n",
    "        # forward modeling\n",
    "        with torch.no_grad():\n",
    "            pMask = networktest(R0t)\n",
    "        # record the sampled testing patches for later display\n",
    "        for c,p in cp:\n",
    "            Tepred[c] = pMask[p][0].cpu().detach().numpy()\n",
    "        # save pMask for final combination\n",
    "        teMasks[Np-bs:Np] = pMask.detach()\n",
    "\n",
    "    # display the sampled patch fitting in validating dataset\n",
    "    pss.view2d(Tepred)\n",
    "    # combine pMask\n",
    "    teMasks = teMasks.squeeze()\n",
    "    pMask_cb = patch_combine_2D(teMasks,test,ixswitch=8070)\n",
    "    # save pMask\n",
    "    pMask_cb.tofile(f'{root_test}/tsMask.dat')\n",
    "    teMasks.numpy().tofile(f'{root_test}/ts_patchMask.dat')\n",
    "    if year[:4] == '2010':\n",
    "        # calculate BCE loss for 2010 data\n",
    "        pMask_cb[pMask_cb>1.0] = 1.0\n",
    "        tmp = F.binary_cross_entropy(torch.tensor(pMask_cb),torch.tensor(masks))\n",
    "        np.array(tmp).tofile(f'{root_test}/BCE_loss.dat')\n",
    "        print(f'The prediction BCE loss for {year} is {tmp}!')\n",
    "    # display the combined prediction in 3D\n",
    "    fig = plt.figure(figsize=(9,7))\n",
    "    ax = fig.add_subplot(1,1,1,projection='3d')\n",
    "    _ = show3D(pMask_cb,ax=ax,xyzi=(test.DD[0]//2,test.DD[1]//2,test.DD[2]//2),\n",
    "               clim=[0,1],rcstride=(5,5),tl=f'Mask_pred')\n",
    "    plt.show()\n",
    "    # display the combined prediction in slices\n",
    "    print(f'Horizontal slice Artifact above 600 ms for {year}:')\n",
    "    plt.imshow(np.max(pMask_cb[:,:,:300],axis=2),vmin=0,vmax=1,aspect=1,cmap='gray')\n",
    "    plt.show()\n",
    "    print(f'Horizontal slice Artifact below 1200 ms for {year}:')\n",
    "    plt.imshow(np.max(pMask_cb[:,:,600:],axis=2),vmin=0,vmax=1,aspect=1,cmap='gray')\n",
    "    plt.show()\n",
    "    print(f'Inline assemble No. 134 for {year}:')\n",
    "    plt.imshow(pMask_cb[134,:,:].T,vmin=0,vmax=1,aspect=0.3,cmap='gray')\n",
    "    plt.show()\n",
    "    if year[:4] == '2010':\n",
    "        print(f'Reference Inline assemble No. 134 for {year}:')\n",
    "        plt.imshow(masks[134,:,:].T,vmin=0,vmax=1,aspect=0.3,cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "224.417px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
